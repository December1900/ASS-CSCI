{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. multinomial logistic regression with mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist.load_data()\n",
    "mnist = input_data.read_data_sets(\"./data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.532864098\n",
      "Epoch: 0002 cost= 0.361025149\n",
      "Epoch: 0003 cost= 0.330213694\n",
      "Epoch: 0004 cost= 0.316047758\n",
      "Epoch: 0005 cost= 0.308028520\n",
      "Epoch: 0006 cost= 0.301304304\n",
      "Epoch: 0007 cost= 0.294422028\n",
      "Epoch: 0008 cost= 0.291574168\n",
      "Epoch: 0009 cost= 0.286512886\n",
      "Epoch: 0010 cost= 0.288759521\n",
      "Epoch: 0011 cost= 0.278943424\n",
      "Epoch: 0012 cost= 0.281038634\n",
      "Epoch: 0013 cost= 0.278534079\n",
      "Epoch: 0014 cost= 0.275540369\n",
      "Epoch: 0015 cost= 0.276986441\n",
      "Epoch: 0016 cost= 0.272276273\n",
      "Epoch: 0017 cost= 0.271651880\n",
      "Epoch: 0018 cost= 0.270200381\n",
      "Epoch: 0019 cost= 0.270604570\n",
      "Epoch: 0020 cost= 0.268859776\n",
      "Finished!\n",
      "Accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.09\n",
    "training_epochs = 20\n",
    "batch_size = 80\n",
    "display_step = 1\n",
    "\n",
    "# mnist data image of shape 28*28=784\n",
    "# 0 - 9 recognition -> 10 classes\n",
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# softmax model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) \n",
    "\n",
    "# using cross entropy\n",
    "# use gradient descent\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Finished!\")\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/sd/0bh7qhmx51x3j987njckxmhm0000gn/T/tmpnwjhrtja\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/sd/0bh7qhmx51x3j987njckxmhm0000gn/T/tmpnwjhrtja', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12d870290>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/sd/0bh7qhmx51x3j987njckxmhm0000gn/T/tmpnwjhrtja/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.327911, step = 1\n",
      "INFO:tensorflow:global_step/sec: 7.61574\n",
      "INFO:tensorflow:loss = 0.15145534, step = 101 (13.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.72179\n",
      "INFO:tensorflow:loss = 0.051528998, step = 201 (11.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.88113\n",
      "INFO:tensorflow:loss = 0.031824302, step = 301 (11.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.08324\n",
      "INFO:tensorflow:loss = 0.09830254, step = 401 (12.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.0937\n",
      "INFO:tensorflow:loss = 0.10937311, step = 501 (10.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.04482\n",
      "INFO:tensorflow:loss = 0.10964825, step = 601 (11.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.0288\n",
      "INFO:tensorflow:loss = 0.00770758, step = 701 (11.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.00612\n",
      "INFO:tensorflow:loss = 0.024043616, step = 801 (11.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.96115\n",
      "INFO:tensorflow:loss = 0.044971745, step = 901 (11.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.01532\n",
      "INFO:tensorflow:loss = 0.10805416, step = 1001 (11.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.96752\n",
      "INFO:tensorflow:loss = 0.03765335, step = 1101 (11.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.57633\n",
      "INFO:tensorflow:loss = 0.0315446, step = 1201 (21.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.99598\n",
      "INFO:tensorflow:loss = 0.0148741985, step = 1301 (25.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.8007\n",
      "INFO:tensorflow:loss = 0.0047245603, step = 1401 (17.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.73326\n",
      "INFO:tensorflow:loss = 0.05966598, step = 1501 (17.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.29545\n",
      "INFO:tensorflow:loss = 0.0069268397, step = 1601 (18.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.31866\n",
      "INFO:tensorflow:loss = 0.021462442, step = 1701 (12.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.02541\n",
      "INFO:tensorflow:loss = 0.0070158895, step = 1801 (11.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.13099\n",
      "INFO:tensorflow:loss = 0.009945361, step = 1901 (10.951 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/sd/0bh7qhmx51x3j987njckxmhm0000gn/T/tmpnwjhrtja/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03316797.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-05-16T02:56:19Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sd/0bh7qhmx51x3j987njckxmhm0000gn/T/tmpnwjhrtja/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-05-16-02:56:22\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9908, global_step = 2000, loss = 0.03127469\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /var/folders/sd/0bh7qhmx51x3j987njckxmhm0000gn/T/tmpnwjhrtja/model.ckpt-2000\n",
      "Testing Accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./data\", one_hot=False)\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "num_input = 784 \n",
    "num_classes = 10 \n",
    "dropout = 0.3\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        x = x_dict['images']\n",
    "\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # 32 filters and 5 kernels in Convolution Layer\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # 64 filters and 3 kernels in Convolution Layer2\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False,\n",
    "                            is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True,\n",
    "                           is_training=False)\n",
    "\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                  global_step=tf.train.get_global_step())\n",
    "\n",
    "\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_op,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs\n",
    "\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "\n",
    "model.train(input_fn, steps=num_steps)\n",
    "\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "e = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cell Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Line Width and Font Size of Plots\n",
    "plt.rcParams['lines.linewidth'] = 1.0\n",
    "plt.rcParams['font.size'] = 6.0\n",
    "plt.rcParams['axes.titlesize'] = 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-0722299bf62d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m                              \u001b[0;34m'but the Sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                              'has length {length}'.format(idx=idx,\n\u001b[0;32m---> 57\u001b[0;31m                                                           length=len(self)))\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_batches_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 212.4x212.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_size = 72\n",
    "batch_size = 20\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('./training',\n",
    "                                                 target_size = (img_size, img_size),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary',\n",
    "                                                 color_mode = 'rgb')\n",
    " \n",
    "test_set = test_datagen.flow_from_directory('./test',\n",
    "                                            target_size = (img_size, img_size),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary',\n",
    "                                            color_mode = 'rgb')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitd11558ffa9074fcd81c84b10d5108c82"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
